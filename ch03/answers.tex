\documentclass[12pt]{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}
\usetikzlibrary{automata,positioning}
\usepackage{enumitem}
%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}

\rhead{\hmwkClass\ : \hmwkTitle}

\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \stepcounter{#1}
    \nobreak\extramarks{Problem \arabic{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}

%
% Homework Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
\newenvironment{homeworkProblem}[1][-1]{
    \ifnum#1>0
        \setcounter{homeworkProblemCounter}{#1}
    \fi
    \section{Exercise \arabic{homeworkProblemCounter}}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}

%
% Homework Details
%   - Title
%   - Due date
%   - Class
%   - Section/Time
%   - Instructor
%   - Author
%

\newcommand{\hmwkTitle}{Chapter 3}
\newcommand{\hmwkDueDate}{January 2,2016}
\newcommand{\hmwkClass}{An Introduction to Statistical Learning}
\newcommand{\hmwkClassTime}{}
\newcommand{\hmwkClassInstructor}{G. James et.al.}
\newcommand{\hmwkAuthorName}{Tony Jiang}

%
% Title Page
%

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate\ at 3:10pm}\\
    \vspace{0.1in}\large{\textit{\hmwkClassInstructor\ \hmwkClassTime}}
    \vspace{3in}
}

\author{\textbf{\hmwkAuthorName}}
\date{}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}


%change enumerate line spacing

\begin{document}

\maketitle

\pagebreak
1. Describe the null hypotheses to which the p-values given in Table 3.4 correspond. Explain what conclusions you can draw based on these p-values. Your explanation should be phrased in terms of  \textcolor{red} {sales},  \textcolor{red} {TV}, \textcolor{red} {radio}, and  \textcolor{red} {newspaper}, rather in terms of the coefficients of the linear model. \\

\begin{table}[ht]
\centering
\begin{tabular}{  l | c | c | c |c}
\hline
& Coefficient&Std. error &t-statistic&p-value\\
\hline 
\textcolor{red}{Intercept}& 2.939&0.3119&9.42& $<$0.0001\\
\textcolor{red}{TV}& 0.046&0.0014&32.81& $<$0.0001\\
\textcolor{red}{radio}& 0.189&0.0086&21.89& $<$0.0001\\
\textcolor{red}{newspaper}& -0.001&0.0059&-0.18& $<$0.8599\\
\hline                    
\end{tabular}
\end{table}
\textbf{Answer:}\\      
$H_0$: \textcolor{red}{TV} ads has no effect on sales. A small p-value makes us reject the null hypotheses and accept the alternative, TV ads has a significant effect on sales. \\
$H_0$: \textcolor{red}{radio} ads has no effect on sales. A small p-value makes us reject the null hypotheses and accept the alternative, TV ads has a significant effect on sales. \\
$H_0$: \textcolor{red}{newspaper} ads has no effect on sales. A large p-value makes us retain the null hypotheses. Newspaper ads has no effect on sales. \\

2. Carefully explain the differences between the KNN classifier and KNN regression methods. \\
\textbf{Answer:}\\
The underlying algorithms are the same but the outputs are different: KNN classifier gives a qualitative output while KNN regression methods yields a quantitative one.\\

3. Suppose we have a data set with five predictors, $X_1=$GPA, $X_2=$IQ,$X_3=$Gender (1 for Female and 0 for Male),$X_4=$Interaction between GPA and IQ, and $X_5$=Interaction between GPA and Gender. The response is starting salary after graduation (in thousands of dollars). Suppose we use least squares to fit the model, and get $\hat{\beta}_0$=50,$\hat{\beta}_1$=20,$\hat{\beta}_2$=0.07,$\hat{\beta}_3$=35,$\hat{\beta}_4$=0.01,$\hat{\beta}_5=-10$. \\
(a) Which answer is correct, and why?
\begin{enumerate}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex,label=\roman*]
 \item For a fixed value of IQ and GPA, males earn more on average than females.
 \item For a fixed value of IQ and GPA, females earn more on average than males.
 \item For a fixed value of IQ and GPA, males earn more on average than females provided that the GPA is high enough.
 \item For a fixed value of IQ and GPA, females earn more on average than males provided that the GPA is high enough.
 \end{enumerate}
(b) Predict the salary of a female with IQ of 110 and a GPA of 4.0. \\
(c) True or false: Since the coefficient for the GPA/IQ interaction term is very small, there is very little evidence of an interaction effect. Justify your answer. \\
\textbf{Answer:}\\
(a) iii is correct. \\
From the fitted model, we have 
\begin{center}
$Sales=50+20GPA+0.07IQ+35Gender+0.01GPA\times IQ-10GPA\times Gender$
\end{center}
So the difference of salary between Female and Male can be expressed by:
\begin{center}
$Sales_{Female}-Sales_{male}=\big(50+20GPA+0.07IQ+35\times 1+0.01GPA\times IQ-10GPA\times 1\big)-\big(50+20GPA+0.07IQ+35\times 0+0.01GPA\times IQ-10GPA\times 0\big)$
\end{center}
Rearrange:
\begin{center}
$Sales_{Female}-Sales_{male}=35-10GPA$
\end{center}
We can easily see that if GPA>3.5, Female will make less salary than male on average. So the answer is iii. \\
(b) Substituting the IQ and GPA into the formula in (a), we can show that the salary will be \textdollar 137.1K.\\
(c) False. The significance of an effect depends on the p-value associated with the testing of the hypothesis on the parameter not on the value of the parameter itself. \\

4. I collect a set of data ($n=$100 observations) containing a single predictor and a quantitative response. I then fit a linear regression model to the data, as well as a separate cubic regression, i.e. $Y=\hat{\beta}_0+\hat{\beta}_1X+\hat{\beta}_X^2+\hat{\beta}_3X^3+\epsilon$
(a) Suppose that the true relationship between X and Y is linear, i.e. $Y=\beta_0+\beta_1X+\epsilon$. Consider the training residual sum of squares (RSS) for the linear regression, and also the training RSS for the cubic regression. Would we expect them to be the same, or is there not enough information to tell? Justify your answer. \\
(b) Answer (a) using test rather than training RSS.\\
(c) Suppose that the true relationship between X and Y is not linear, but we don't know how far it is from linear. Consider the training RSS for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer. \\
(d) Answer(c) using test rather than training RSS.\\
\textbf{Answer:}\\
(a) cubic regression will have a smaller training RSS than linear model because cubic has more flexibility and can provide a better fit.\\
(b) cubic regression will have a larger test RSS than linear model as there is really no non-linear relationship thus cubic regression over fit the model. \\
(c) Regardless the linearity in the true relationship, cubic regression will have a smaller training RSS than linear model because cubic has more flexibility and can provide a better fit.\\
(d) More information are required.\\
 
5. Consider the fitted values that result from performing linear regression without an intercept. In this setting, the $ith$ fitted value takes the form
\begin{center}
$\hat{y_i}=x_i \hat{\beta},$
\end{center}
where
$$\hat{\beta}=\Big(\sum_{i=1}^n {x_i y_i} \Big)/ \Big(\sum_{i'=1}^n {x_{i'}^2} \Big) $$
Show that we can write 
$$\hat{y_i}=\sum_{i'=1}^n{a_{i'}y_{i'}} $$
What is $a_{i'}$?\\
\textit{Note: We interpret this result by saying that the fitted values from linear regression are linear combinations of the response values.}\\
\textbf{Answer:}\\
Substitute $\hat{\beta}$ back to the first equation
\begin{align*} 
\hat{y_i}&=x_i\hat{\beta}\\
&=x_i\Big(\sum_{i=1}^n {x_i y_i} \Big)/ \Big(\sum_{i'=1}^n {x_{i'}^2} \Big)\\
&=\frac{x_i\sum_{i'=1}^n {x_{i'} y_{i'}} }{\sum_{i'=1}^n {x_{i'}^2}}\\
&=\sum_{i'=1}^n {\frac{x_i x_{i'}}{\sum_{i'=1}^n {x_{i'}^2} }} y_{i'}
\end{align*}
we can observe that $a_{i'}=\frac{x_i x_{i'}}{\sum_{i'=1}^n {x_{i'}^2} }$\\

6. Using (3.4), argue that in the case of simple linear regression, the least squares line always passes through the point $(\bar{x},\bar{y})$.\\
\textbf{Answer:}\\
We first take summation on all fitted values then divide by n. With some rearrangement we can easily show that ($\bar{y}$,$\bar{x}$) also satisfy the equation defined by the line with intercept of $\hat{\beta_0}$ and a slope of $\hat{\beta_1}$.
\begin{align*}
y_i&=\hat{\beta_0}+\hat{\beta_1}x_i\\
\sum_{i=1}^n y_i&=\sum_{i=1}^n \big(\hat{\beta_0}+\hat{\beta_1}x_i \big)\\
\frac{1}{n} \sum_{i=1}^n y_i &=\frac{1}{n}\sum_{i=1}^n \big(\hat{\beta_0}+\hat{\beta_1}x_i \big)\\
\bar{y}&=\hat{\beta_0}+\hat{\beta_1}\bar{x}
\end{align*}

7. It is claimed in the text that in the case of simple linear regression of Y onto X, the $R^2$ statistic (3.17) is equal to the square of the correlation between X and Y (3.18). Prove that this is the case. For simplicity, you may assume that $\bar{x}=\bar{y}=0$. \\
\textbf{Answer:}\\
\begin{align*}
R^2&=1-\frac{RSS}{TSS}\\
&=1-\frac{\sum{(y_i-\hat{\beta}x_i)^2}}{\sum {(y_i-\bar{y})}^2}\\
&=1-\frac{\sum{(y_i-\hat{\beta}x_i)^2}}{\sum {y_i}^2}\\
&=\frac{\sum {y_i}^2-\sum{(y_i-\hat{\beta}x_i)^2}}{\sum {y_i}^2}\\
&=\frac{\sum {y_i}^2-\sum{y_i^2}+2\hat{\beta}\sum{x_i y_i}-\hat{\beta}^2 \sum{x_i^2}}{\sum {y_i}^2}\\
&=\frac{\sum {y_i}^2-\sum{y_i^2}+2\hat{\beta}\sum{x_i (x_i \hat{\beta})}-\hat{\beta}^2 \sum{x_i^2}}{\sum {y_i}^2}\\
&=\frac{\hat{\beta}^2 \sum{x_i^2}}{\sum {y_i}^2}\\
&=\frac{\hat{\beta}^2 \sum{x_i^2}}{\sum {y_i}^2}\\
&=\frac{(\frac{\sum x_i y_i}{\sum x_i^2})^2 \sum{x_i^2}}{\sum {y_i}^2}\\
&=\frac{(\sum x_i y_i)^2 }{\sum{x_i^2} \sum {y_i}^2}\\
&=\Big( \frac{\sum x_i y_i}{\sum x_i^2 \sum y_i^2}\Big)^2\\
&=\Big( Cor(x,y) \Big)^2
\end{align*}
Note: this only applies to simple linear regression model with no intercept (so we only need to deal with one $\beta$. \\

\end{document}
