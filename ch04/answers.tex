\documentclass[12pt]{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}
\usetikzlibrary{automata,positioning}
\usepackage{enumitem}
%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}

\rhead{\hmwkClass\ : \hmwkTitle}

\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \stepcounter{#1}
    \nobreak\extramarks{Problem \arabic{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}

%
% Homework Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
\newenvironment{homeworkProblem}[1][-1]{
    \ifnum#1>0
        \setcounter{homeworkProblemCounter}{#1}
    \fi
    \section{Exercise \arabic{homeworkProblemCounter}}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}

%
% Homework Details
%   - Title
%   - Due date
%   - Class
%   - Section/Time
%   - Instructor
%   - Author
%

\newcommand{\hmwkTitle}{Chapter 4}
\newcommand{\hmwkDueDate}{January 5,2016}
\newcommand{\hmwkClass}{An Introduction to Statistical Learning}
\newcommand{\hmwkClassTime}{}
\newcommand{\hmwkClassInstructor}{G. James et.al.}
\newcommand{\hmwkAuthorName}{Tony Jiang}

%
% Title Page
%

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate\ at 3:10pm}\\
    \vspace{0.1in}\large{\textit{\hmwkClassInstructor\ \hmwkClassTime}}
    \vspace{3in}
}

\author{\textbf{\hmwkAuthorName}}
\date{}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}


%change enumerate line spacing

\begin{document}

\maketitle

\pagebreak
1. Using a little bit of algebra, prove that (4.2) is equivalent to (4.3). In other words, the logistic function representation and logit representation for the logistic regression model are equivalent. \\
\textbf{Answer:}\\
\begin{align*}
p(X)&=\frac{e^{\beta_0+\beta_1X}}{1+e^{\beta_0+\beta_1X}}\quad (4.2)\\
\frac{p(X)}{1-p(X)}&=\frac{\frac{e^{\beta_0+\beta_1X}}{1+e^{\beta_0+\beta_1X}}}{1-\frac{e^{\beta_0+\beta_1X}}{1+e^{\beta_0+\beta_1X}}}\\
&=\frac{\frac{e^{\beta_0+\beta_1X}}{1+e^{\beta_0+\beta_1X}}}{\frac{1+e^{\beta_0+\beta_1X}-e^{\beta_0+\beta_1X}}{1+e^{\beta_0+\beta_1X}}}\\
&=e^{\beta_0+\beta_1X} \quad (4.3)\\
\end{align*}
2. It was stated in the text that classifying an observation to the class for which (4.12) is largest is equivalent to classifying an observation to the class for which (4.13) is largest. Prove that this is the case. In other words, under the assumption that the observations in the kth class are drawn from a $N(\mu_k,\sigma^2)$ distribution, the Bayes' classifier assigns an observation to the class for which the discriminant function is maximized.\\
\textbf{Answer:}\\
Recall 4.12 has the form:
\begin{align*}
p_k(x)&=\frac{\pi_k\frac{1}{\sqrt{2\pi}\sigma}exp\big(-\frac{1}{2\sigma^2}(x-\mu_k)^2\big)}{\sum_{l=1}^K{}\pi_l\frac{1}{\sqrt{2\pi}\sigma}exp\big(-\frac{1}{2\sigma^2}(x-\mu_l)^2\big)}
\end{align*}
and 4.13
\begin{align*}
\delta_k(x)=x \cdot \frac{\mu_k}{\sigma^2}-\frac{\mu_k}{\sigma^2}-\frac{\mu_k^2}{2\sigma^2}+log(\pi_k)
\end{align*}
Taking the log of 4.12 on both sides:
\begin{align*}
log\big(p_k(x)\big)&=log\Big(\frac{\pi_k\frac{1}{\sqrt{2\pi}\sigma}exp\big(-\frac{1}{2\sigma^2}(x-\mu_k)^2\big)}{\sum_{l=1}^K{}\pi_l\frac{1}{\sqrt{2\pi}\sigma}exp\big(-\frac{1}{2\sigma^2}(x-\mu_l)^2\big)}\Big)\\
&=log\Big(\pi_k\frac{1}{\sqrt{2\pi}\sigma}exp\big(-\frac{1}{2\sigma^2}(x-\mu_k)^2\big)\Big)-log\Big( \sum_{l=1}^K{}\pi_l\frac{1}{\sqrt{2\pi}\sigma}exp\big(-\frac{1}{2\sigma^2}(x-\mu_l)^2\big)\Big)\\
\end{align*}
To maximize this , we only need to maximize all items related to class k. that is to say, we only care about what is a function of k (the rest are the same to a given x).
\begin{align*}
\arg\max_x{p_k(x)}&=\arg\max_x{log(p_k(x))}\\
&=\arg\max_x \quad log\Big(\pi_k\frac{1}{\sqrt{2\pi}\sigma}exp\big(-\frac{1}{2\sigma^2}(x-\mu_k)^2\big)\Big)\\
&=\arg\max_x \quad \Big(log(\pi_k)+log(\frac{1}{\sqrt{2\pi}\sigma})-[\frac{1}{2\sigma^2}(x^2-2\mu_kx+\mu_k^2)]\Big)\\
&=\arg\max_x \quad \Big(log(\pi_k)-\frac{x^2}{2\sigma^2}+\frac{2\mu_kx}{2\sigma^2}-\frac{\mu_k^2}{2\sigma^2}\Big)\\
&=\arg\max_x \quad \Big(log(\pi_k)-\frac{x^2}{2\sigma^2}+\frac{\mu_kx}{\sigma^2}-\frac{\mu_k^2}{2\sigma^2}\Big)\\
&=\arg\max_x\quad\Big(x\cdot\frac{\mu_k}{\sigma^2}-\frac{\mu_k^2}{2\sigma^2}+log(\pi_k)\Big)\\
\end{align*}
The last line bears the same form as 4.13.\\

3. This problem relates to the QDA model,in which the observations within each class are drawn from a normal distribution with a class-specific mean vector and a class specific covariance matrix. We consider the simple case where $p=1$; i.e. there is only one feature.\\

Suppose that we have $K$ classes, and that if an observation belongs to the $k$th class then $X$ comes form a one-dimensional normal distribution, $X~N(\mu_k,\sigma_k^2)$. Recall that the density function for the one-dimensional normal distribution is given in (4.11). Prove that in this case, the Bayes' classifier is \textit{not} linear. Argue that it is in fact quadratic. \\

Hint: For this problem, you should follow the arguments laid out in Section 4.4.2, but without making the assumption that $\sigma_1^2=...=\sigma_K^2.$ \\

\textbf{Answer:}\\
\begin{align*}
\arg\max_x{p_k(x)}&=\arg\max_x{log(p_k(x))}\\
&=\arg\max_x \quad log\Big(\pi_k\frac{1}{\sqrt{2\pi}\sigma_k}exp\big(-\frac{1}{2\sigma_k^2}(x-\mu_k)^2\big)\Big)\\
&=\arg\max_x \quad \Big(log(\pi_k)+log(\frac{1}{\sqrt{2\pi}\sigma_k})-[\frac{1}{2\sigma_k^2}(x^2-2\mu_kx+\mu_k^2)]\Big)\\
&=\arg\max_x \quad \Big(log(\pi_k)-\frac{x^2}{2\sigma_k^2}+\frac{2\mu_kx}{2\sigma_k^2}-\frac{\mu_k^2}{2\sigma_k^2}\Big)\\
&=\arg\max_x \quad \Big(log(\pi_k)-\frac{x^2}{2\sigma_k^2}+\frac{\mu_kx}{\sigma_k^2}-\frac{\mu_k^2}{2\sigma_k^2}\Big)\\
&=\arg\max_x \quad \Big(-x^2\cdot \frac{1}{2\sigma_k^2}+x\cdot \frac{\mu_k}{\sigma_k^2}-\frac{\mu_k^2}{2\sigma_k^2}+log(\pi_k)\Big)\\
\end{align*}
It does contain the $x^2$ term. 
\end{document}
